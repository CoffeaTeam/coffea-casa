distributed:
    version: 2
  
    dashboard:
      link: /user/{JUPYTERHUB_USER}/proxy/{port}/status
  
    scheduler:
      idle-timeout: 3600s
  
  # uncomment to force new worker pods after 2 hrs
  #  worker:
  #    lifetime:
  #      duration: "2 hours"
  #      stagger: "10 s"
  #      restart: true
  
    admin:
      tick:
        limit: 5s
  
    logging:
      distributed: warning
      bokeh: critical
      tornado: critical
      tornado.application: error

# Part of kubernetes.yml
kubernetes:
    name: "dask-unl"
    namespace: default
    count:
        start: 0
        max: null
    host: "0.0.0.0"
    port: 0
    env: {}
    idle-timeout: null
    deploy-mode: "local"
    interface: null
    protocol: "tls://"
    dashboard_address: ":8787"
    scheduler-service-type: "ClusterIP"
    # Timeout to wait for the scheduler service to be up (in seconds)
    # Set it to 0 to wait indefinitely (not recommended)
    scheduler-service-wait-timeout: 30
    scheduler-service-template:
        apiVersion: v1
        kind: Service
        spec:
        selector:
            dask.org/cluster-name: "" # Cluster name will be added automatically
            dask.org/component: scheduler
        ports:
            - name: comm
            protocol: TLS
            port: 8786
            targetPort: 8786
            - name: dashboard
            protocol: TLS
            port: 8787
            targetPort: 8787
    scheduler-template: {}
    worker-template:
        kind: Pod
        metadata:
        labels:
            app: "dask"
        spec:
        restartPolicy: Never
        nodeSelector:
            beta.kubernetes.io/os: linux
        containers:
        - name: dask
            image: oshadura/coffea-casa:0.1.9
            env:
            - name: EXTRA_PIP_PACKAGES
            value: "joblib scikit-learn stumpy"
            args:
            - dask-worker
            - --nthreads
            - '4'
            - --no-bokeh
            - --memory-limit
            - 5GB
            - --death-timeout
            - '60'
            resources:
            limits:
                cpu: "1"
                memory: 6G
            requests:
                cpu: "1"
                memory: 5G

# Specific to each hub
#gateway:
  #  address: xxxx/services/dask-gateway/
  #  proxy-address: tls://xx.xxx.xx:8786
  
labextension:
  factory:
    module: 'dask_jobqueue'
    class: 'HTCondorCluster'
    args: []
    kwargs: {cores: 4,
              memory: "2GB",
              disk: 1GB",
              log_directory: "logs",
              silence_logs: "debug",
              scheduler_options: {"dashboard_address":"8786","port":8787, "external_address": "129.93.183.33:8787"},
              job_extra: {"universe": "docker",
                                     "encrypt_input_files": "/etc/cmsaf-secrets/xcache_token",
                                     "docker_image": "coffeateam/coffea-casa-analysis:0.1.11", 
                                     "container_service_names": "dask",
                                     "dask_container_port": "8787",
                                     "should_transfer_files": "YES",
                                     "when_to_transfer_output": "ON_EXIT",
                                     "+DaskSchedulerAddress": '"129.93.183.33:8787"',}}
                                     
  default:
    workers: null
    adapt: null
      minimum: 2
      maximum: 10

  initial:
    - name: "default"
      workers: 1
      adapt:
        minimum: 1
        maximum: 100
  