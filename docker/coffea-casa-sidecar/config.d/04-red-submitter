##############################################################################
##############################################################################
#
#	DO NOT EDIT - file is being maintained by puppet
#
##############################################################################
##############################################################################


START   = FALSE
SUSPEND = FALSE
PREEMPT = FALSE
KILL    = FALSE

DAEMON_LIST = MASTER, SCHEDD

USE_PSS = TRUE

########
### FIXME For K8s we remove IPV6!
########
#ENABLE_IPV6 = TRUE

# K8s sched setup
SCHEDD_HOST = t3.unl.edu

# define max interval for evaluating periodic release/removal
MAX_PERIODIC_EXPR_INTERVAL = 1200

# hold running jobs using absurd amounts of disk (100+ GB) or using more memory than requested.
SYSTEM_PERIODIC_HOLD = \
   JobStatus == 2 && ((DiskUsage > 500000000 || ResidentSetSize > 1050*RequestMemory))

# Report why the stupid thing went on hold.
SYSTEM_PERIODIC_HOLD_REASON = strcat("Job in status ", JobStatus, " put on hold by SYSTEM_PERIODIC_HOLD due to ", ifThenElse(isUndefined(DiskUsage) || DiskUsage < 100000000, strcat("memory usage ", ResidentSetSize), strcat("disk usage ", DiskUsage)), ".")

# forceful removal of running after 9 days, held jobs after 6 hours,
# and anything trying to run more than 10 times
SYSTEM_PERIODIC_REMOVE = \
   (JobStatus == 5 && CurrentTime - EnteredCurrentStatus > 3600*6) || \
   (JobStatus == 2 && CurrentTime - EnteredCurrentStatus > 3600*24*9) || \
   (JobStatus == 5 && JobRunCount >= 10) || \
   (JobStatus == 5 && HoldReasonCode =?= 14 && HoldReasonSubCode =?= 2)

# Record why the job was removed
SYSTEM_PERIODIC_REMOVE_REASON = strcat("Job removed by SYSTEM_PERIODIC_REMOVE due to ", \
   ifThenElse((JobStatus == 5 && CurrentTime - EnteredCurrentStatus > 3600*6), "being in hold state for 6 hours", \
   ifThenElse((JobStatus == 2 && CurrentTime - EnteredCurrentStatus > 3600*24*4), "runtime of longer than 96 hours", \
   ifThenElse((JobStatus == 5 && JobRunCount >= 10), "more than 10 restarts", \
              "input files missing"))), ".")

# Release from hold a few times
# HoldReasonCode 13 and 14 have to do with failures to upload
SYSTEM_PERIODIC_RELEASE = \
  (   (HoldReasonCode =?= 12) \
   || (HoldReasonCode =?= 13) \
   || (HoldReasonCode =?= 14 && HoldReasonSubCode =!= 2)) \
   && ((CurrentTime - EnteredCurrentStatus > 5*60) \
   && (JobRunCount < 10))

EVENT_LOG                          = $(LOG)/EventLog
EVENT_LOG_USE_XML                  = True
EVENT_LOG_JOB_AD_INFORMATION_ATTRS = Owner,CurrentHosts,x509userproxysubject,AccountingGroup,GlobalJobId,QDate,JobStartDate,JobCurrentStartDate,JobFinishedHookDone,RemoteHost

##  Maximum number of simultaneous downloads of output files from
##  execute machines to the submit machine (limit applied per schedd).
##  The value 0 means unlimited.
MAX_CONCURRENT_DOWNLOADS = 50

##  Maximum number of simultaneous uploads of input files from the
##  submit machine to execute machines (limit applied per schedd).
##  The value 0 means unlimited.
MAX_CONCURRENT_UPLOADS = 50

# Next line is commented out because gratia RPM now sets this value too.
# PER_JOB_HISTORY_DIR = /var/lib/gratia/data

MAX_HISTORY_LOG = 104857600
MAX_HISTORY_ROTATIONS = 10

## Configure integration with QPID
QMF_PUBLISH_SUBMISSIONS = FALSE
QMF_BROKER_USERNAME = submitter
QMF_BROKER_PASSWORD_FILE = /etc/condor/qpid_passfile_submitter
QMF_BROKER_AUTH_MECH = PLAIN
QMF_BROKER_HOST = red-web.unl.edu
QMF_BROKER_PORT = 5672
ENABLE_RUNTIME_CONFIG = TRUE

USE_CLONE_TO_CREATE_PROCESSES = false

#
# JobRouter, thy name is legion...
#

DAEMON_LIST = $(DAEMON_LIST) JOB_ROUTER

JOB_ROUTER_ENTRIES = \
   [ \
     Requirements = target.JobUniverse == 5 && target.HasLocalCustomizations =!= true; \
     EditJobInPlace = true; \
     /* JobRouter chokes if this is not set, although it's obviously meaningless. */ \
     GridResource = "condor local local"; \
     eval_set_IsT3User = regexp("cms.other.t3.*", ifThenElse(isUndefined(AccountingGroup), "", AccountingGroup)); \
     /* Append extra requirements. */ \
     copy_Requirements = "OriginalRequirements"; \
     set_Requirements = (my.OriginalRequirements) && (my.HasLocalCustomizations =?= true) && (target.IS_GLIDEIN=!=TRUE || target.GLIDECLIENT_Group=?="T2Overflow"); \
     set_HasLocalCustomizations = true; \
     set_WantIOProxy = true; \
     set_RequestedChroot = "SL6"; \
     /* Some ClassAds tap-dancing to do minimum requests. */ \
     copy_RequestCpus = "OriginalRequestCpus"; \
     eval_set_RequestCpus = ifThenElse(isUndefined(OriginalRequestCpus), 1, OriginalRequestCpus); \
     copy_RequestDisk = "OriginalRequestDisk"; \
#     eval_set_RequestDisk = ifThenElse(OriginalRequestDisk > 10240, OriginalRequestDisk, 10240); \
     copy_RequestMemory = "OriginalRequestMemory"; \
     eval_set_RequestMemory = ifThenElse(regexp("cms\.", ifThenElse(isUndefined(AccountingGroup), "", AccountingGroup)) && OriginalRequestMemory < 2500, 2500, \
                              ifThenElse(OriginalRequestMemory < 2500, 2500, OriginalRequestMemory)); \
#     set_IsT2Overflow = ((CurrentTime - QDate) > 60 * 60 * 24) && (IsT3User == false);  \
     set_IsT2Overflow = FALSE;
     name = "Local_Transform"; \
   ]

JOB_ROUTER_DEFAULTS =  []

JOB_ROUTER_SOURCE_JOB_CONSTRAINT = true

# Additional statistics for Ganglia
STATISTICS_TO_PUBLISH = TRANSFER:2

# Keep the audit log
SCHEDD_AUDIT_LOG = $(LOG)/AuditLog
SCHEDD_DEBUG = D_AUDIT
MAX_SCHEDD_AUDIT_LOG = 1d
MAX_NUM_SCHEDD_AUDIT_LOG = 90

# Add "red-condor" to the queue super-users (that is, the daemons on the collector)
QUEUE_SUPER_USERS = root, condor, red-condor

DELEGATE_JOB_GSI_CREDENTIALS_LIFETIME = 4*86400
SCHEDD_INTERVAL = 60

